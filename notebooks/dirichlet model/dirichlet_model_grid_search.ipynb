{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "import sys\n",
    "from scipy.stats import dirichlet\n",
    "import seaborn as sns\n",
    "sns.set(style='whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_polls_df = pd.read_csv('ground_truth_polling.csv', index_col=0)\n",
    "best_guess_polling_df = pd.read_csv('best_guess_polling.csv', index_col=0)\n",
    "first_valid_day = datetime.date(2019, 3, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_counts_df = pd.read_csv('FINALLL.csv', index_col='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_polls_df.index = pd.to_datetime(ground_truth_polls_df.index)\n",
    "best_guess_polling_df.index = pd.to_datetime(best_guess_polling_df.index)\n",
    "twitter_counts_df.index = pd.to_datetime(twitter_counts_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_valid_day = list(twitter_counts_df.index)[-1].to_pydatetime().date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweet_columns(threshold):\n",
    "    return (['Number of tweets >= ' + str(threshold) + '_' + cand \n",
    "             for cand in list(ground_truth_polls_df.columns)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tweets between latest_poll_date and number of days in advance from that date\n",
    "def get_tweets(latest_poll_date, days_in_advance, columns):\n",
    "    days_not_nan = np.zeros(len(columns))\n",
    "    tweet_tot = np.zeros(len(columns))\n",
    "    for i in range(1, days_in_advance + 1):\n",
    "        tweets = np.array(twitter_counts_df.loc[latest_poll_date + datetime.timedelta(days=i), columns])\n",
    "        days_not_nan += [int(not np.isnan(x)) for x in tweets]\n",
    "        tweet_tot += [0 if np.isnan(x) else x for x in tweets]\n",
    "    \n",
    "    days_not_nan = [days if days else 1 for days in days_not_nan]\n",
    "    return days_in_advance * (tweet_tot / days_not_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update prior distribution using Twitter info and calculate posterior likelihood of ground truth polling \n",
    "def obtain_posterior_likelihood(latest_poll_date, days_in_advance, poll_scaling_factor, decay_factor, \n",
    "                     tweet_scaling_factor):\n",
    "    prediction_date = latest_poll_date + datetime.timedelta(days=days_in_advance)\n",
    "    sum_of_alphas = poll_scaling_factor * decay_factor ** days_in_advance\n",
    "    polling_prior_belief = np.array(best_guess_polling_df.loc[latest_poll_date])\n",
    "    \n",
    "    proportion_top_5 = np.sum(polling_prior_belief)\n",
    "    prior_alphas = np.array([(cand_proportion / proportion_top_5) * sum_of_alphas \n",
    "                             for cand_proportion in polling_prior_belief])\n",
    "    \n",
    "    tweet_data = get_tweets(latest_poll_date, days_in_advance, get_tweet_columns(0.6))\n",
    "    posterior_alphas = prior_alphas + tweet_data * tweet_scaling_factor \n",
    "    ground_truth_polling = np.array(ground_truth_polls_df.loc[prediction_date])\n",
    "    ground_truth_polling_sum_to_1 = ground_truth_polling / np.sum(ground_truth_polling)\n",
    "    \n",
    "    return(dirichlet.pdf(ground_truth_polling_sum_to_1, posterior_alphas))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wfried/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3304: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# performing grid search to find optimal values of poll_scaling_factor and tweet_scaling_factor\n",
    "poll_scaling_factors = [10000 * x for x in range(5, 12)]\n",
    "tweet_scaling_factors = np.linspace(0.1, 2, ((2 - 0.1) / 0.1) + 2) / 100\n",
    "decay_factors = np.linspace(0.995, 0.95, ((0.995 - 0.95) / 0.005) + 1)\n",
    "sentiment_threshold = 0.6\n",
    "\n",
    "days_in_advance = 5\n",
    "\n",
    "starting_days = []\n",
    "curr_day = first_valid_day \n",
    "while curr_day < last_valid_day:\n",
    "    starting_days.append(curr_day)\n",
    "    curr_day += datetime.timedelta(days=days_in_advance)\n",
    "\n",
    "del starting_days[-1]\n",
    "    \n",
    "last_starting_day = starting_days[-1]\n",
    "last_starting_day_days_in_advance = (last_valid_day - last_starting_day).days\n",
    "\n",
    "cv_df = pd.DataFrame(columns=['poll_scaling_factors', 'tweet_scaling_factors', 'likelihood|model'])\n",
    "\n",
    "cnt = 0\n",
    "for poll_scaling_factor in poll_scaling_factors:\n",
    "    for tweet_scaling_factor in tweet_scaling_factors:\n",
    "        likelihood_lst = []\n",
    "        for starting_day in starting_days:\n",
    "                likelihood = obtain_posterior_likelihood(starting_day, days_in_advance, poll_scaling_factor,\n",
    "                                                         1, tweet_scaling_factor)\n",
    "                likelihood_lst.append(likelihood)     \n",
    "        cv_df.loc[cnt] = [poll_scaling_factor, tweet_scaling_factor, np.mean(likelihood_lst)]\n",
    "        cnt += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "poll_scaling_factors     1.100000e+05\n",
       "tweet_scaling_factors    2.000000e-03\n",
       "likelihood|model         1.605145e+08\n",
       "Name: 121, dtype: float64"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract optimal hyperparamters from dataframe\n",
    "cv_df.loc[cv_df['likelihood|model'].idxmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
