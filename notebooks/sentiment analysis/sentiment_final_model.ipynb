{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense, LSTM, Bidirectional\n",
    "from keras import optimizers\n",
    "from keras import losses\n",
    "from keras import metrics\n",
    "from keras import callbacks\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import tokenizer_from_json\n",
    "\n",
    "\n",
    "import io\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentiment as sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_datasets2(proportion=0.5):\n",
    "    data = pd.read_csv('./data/trainingandtestdata/train.csv', encoding='utf-8')\n",
    "    gop_debate = pd.read_csv(\"./data/trainingandtestdata/Sentiment.csv\")\n",
    "    \n",
    "    data = data.rename(columns={'0': 'sentiment', '@switchfoot http://twitpic.com/2y1zl - Awww, that\\'s a bummer.  You shoulda got David Carr of Third Day to do it. ;D': 'tweet'})\n",
    "    data_proc = data.sample(frac=proportion, replace=False)\n",
    "    data_proc = data_proc.loc[:, ['tweet', 'sentiment']]\n",
    "    data_proc.loc[data_proc['sentiment'] == 4, 'sentiment'] = 1\n",
    "    \n",
    "    gop_debate = gop_debate.rename(columns={'text': 'tweet'})\n",
    "    gop_debate_proc = gop_debate.loc[gop_debate['sentiment'] != 'Neutral', ['tweet', 'sentiment']]\n",
    "    gop_debate_proc.loc[gop_debate_proc['sentiment'] == 'Positive', 'sentiment'] = 1\n",
    "    gop_debate_proc.loc[gop_debate_proc['sentiment'] == 'Negative', 'sentiment'] = 0\n",
    "    \n",
    "    data_concat = pd.concat([data_proc, gop_debate_proc], ignore_index=True)\n",
    "    data_concat = data_concat.sample(frac=1, replace=False).reset_index(drop=True)\n",
    "    return data_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_df2 = augment_datasets2(proportion = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1610728, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I dont know what i did</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@MajaPiraja are you looking for a home-based j...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@legallove Yay! Bring your dancing shoes - it'...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@AndrewMoriarty LOL well, I'm pro life, yet I'...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@shweri we are not mad  frank and i both test ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet sentiment\n",
       "0                            I dont know what i did          0\n",
       "1  @MajaPiraja are you looking for a home-based j...         1\n",
       "2  @legallove Yay! Bring your dancing shoes - it'...         1\n",
       "3  @AndrewMoriarty LOL well, I'm pro life, yet I'...         1\n",
       "4  @shweri we are not mad  frank and i both test ...         1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(augmented_df2.shape)\n",
    "augmented_df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove stopwords, emoticons, hashtags and mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'over',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'too',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'should',\n",
       " 'now']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = []\n",
    "with open(\"./data/stopwords.txt\") as f:\n",
    "    lines = f.readlines()\n",
    "for i in range(1,len(lines)):\n",
    "    stopwords.append(lines[i].strip())\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[':-@',\n",
       " '>:o',\n",
       " '>:0',\n",
       " 'D:<',\n",
       " 'D:',\n",
       " 'D8',\n",
       " 'D;',\n",
       " 'D=',\n",
       " 'Dx',\n",
       " '>.<',\n",
       " '>_<',\n",
       " 'd:<',\n",
       " 'd:',\n",
       " 'd8',\n",
       " 'd;',\n",
       " 'd=',\n",
       " 'dx',\n",
       " 'v.v',\n",
       " ':/',\n",
       " ':\\\\',\n",
       " '=/',\n",
       " '=\\\\',\n",
       " '>:/',\n",
       " '>:\\\\',\n",
       " ':-/',\n",
       " ':-\\\\',\n",
       " ':)',\n",
       " '(:',\n",
       " ';)',\n",
       " ';(',\n",
       " '(;',\n",
       " ');',\n",
       " ':-)',\n",
       " ':3',\n",
       " ':d',\n",
       " ':D',\n",
       " 'xd',\n",
       " \":')\",\n",
       " '^_^',\n",
       " '^.^',\n",
       " ':]',\n",
       " ':}',\n",
       " ':p',\n",
       " ':b',\n",
       " '=p',\n",
       " '=b',\n",
       " ':-p',\n",
       " ':-b',\n",
       " '=)',\n",
       " ':(',\n",
       " '):',\n",
       " \":'(\",\n",
       " ':c',\n",
       " ':-(',\n",
       " '</3',\n",
       " ':[',\n",
       " ':{',\n",
       " 'T.T',\n",
       " 'o_o',\n",
       " 'O_O',\n",
       " '0_o',\n",
       " 'o_0',\n",
       " '0_O',\n",
       " 'O_0',\n",
       " 'o.o',\n",
       " 'O.O',\n",
       " '0.o',\n",
       " 'o.0',\n",
       " ':o',\n",
       " ':-o',\n",
       " '<3',\n",
       " ':p',\n",
       " ':b',\n",
       " '=p',\n",
       " '=b',\n",
       " ':-p',\n",
       " ':-b',\n",
       " ':$']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoticons = []\n",
    "with open(\"./data/emoticons.txt\") as f:\n",
    "    lines = f.readlines()\n",
    "for i in range(1,len(lines)):\n",
    "    emoticons.append(lines[i].strip())\n",
    "emoticons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to remove hashtags and mentions\n",
    "#remove stopwords and emoticons\n",
    "#trasform everything to lowercase\n",
    "def preprocess_tweet(tweet):\n",
    "    tweet_lower = tweet.lower()\n",
    "    tweet_words = tweet_lower.split()\n",
    "    toberemoved = []\n",
    "    for word in tweet_words:\n",
    "        if word.startswith('@') or word.startswith('#') or word.startswith('http'):\n",
    "            toberemoved.append(word)\n",
    "        elif word in stopwords or word in emoticons:\n",
    "            toberemoved.append(word)\n",
    "    for word in toberemoved:\n",
    "        tweet_words.remove(word)\n",
    "\n",
    "    return ' '.join(tweet_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i dont know i</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>looking home-based job? would like offer servi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yay! bring dancing shoes - it's gone hardcore ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lol well, i'm pro life, yet i'm christian, roo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>not mad frank i test new ui recently, very pre...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet sentiment\n",
       "0                                      i dont know i         0\n",
       "1  looking home-based job? would like offer servi...         1\n",
       "2  yay! bring dancing shoes - it's gone hardcore ...         1\n",
       "3  lol well, i'm pro life, yet i'm christian, roo...         1\n",
       "4  not mad frank i test new ui recently, very pre...         1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_df2.iloc[:,0] = augmented_df2.iloc[:,0].map(preprocess_tweet)\n",
    "augmented_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    808492\n",
       "1    802236\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_df2['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the tweets and create training/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load tokenizer\n",
    "with open('./data/tokenizer_200k.json') as f:\n",
    "    data = json.load(f)\n",
    "    tokenizer = tokenizer_from_json(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "966436\n",
      "966436\n",
      "322145\n",
      "322145\n",
      "322147\n",
      "322147\n"
     ]
    }
   ],
   "source": [
    "max_words = 200000\n",
    "max_length = 50\n",
    "\n",
    "sequences1 = tokenizer.texts_to_sequences(texts)\n",
    "#set the maximum length of each tweet based on dataset\n",
    "\n",
    "padded_seq1 = pad_sequences(sequences1, maxlen=max_length)\n",
    "labels1 = augmented_df2['sentiment'].values\n",
    "\n",
    "train_proportion = 0.6\n",
    "val_proportion = 0.2\n",
    "\n",
    "\n",
    "x_train1 = padded_seq1[:int(train_proportion*len(padded_seq1))]\n",
    "y_train1 = labels1[:int(train_proportion*len(padded_seq1))]\n",
    "\n",
    "x_val1 = padded_seq1[int(train_proportion*len(padded_seq1)):int(train_proportion*len(padded_seq1))+int(val_proportion*len(padded_seq1))]\n",
    "y_val1 = labels1[int(train_proportion*len(padded_seq1)):int(train_proportion*len(padded_seq1))+int(val_proportion*len(padded_seq1))]\n",
    "\n",
    "x_test1 = padded_seq1[int(train_proportion*len(padded_seq1))+int(val_proportion*len(padded_seq1)):]\n",
    "y_test1 = labels1[int(train_proportion*len(padded_seq1))+int(val_proportion*len(padded_seq1)):]\n",
    "\n",
    "\n",
    "print(len(x_train1))\n",
    "print(len(y_train1))\n",
    "print(len(x_val1))\n",
    "print(len(y_val1))\n",
    "print(len(x_test1))\n",
    "print(len(y_test1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the model and load pre-trained neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "\n",
    "lstm_model4 = Sequential()\n",
    "lstm_model4.add(Embedding(max_words, embedding_dim, input_length=max_length))\n",
    "lstm_model4.add(LSTM(64, return_sequences=True))\n",
    "lstm_model4.add(LSTM(32))\n",
    "lstm_model4.add(Dense(32, activation='relu'))\n",
    "#output layer\n",
    "lstm_model4.add(Dense(1, activation='sigmoid'))\n",
    "lstm_model4.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "lstm_model4.load_weights('LSTM_model5_nostop.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on training, validation and test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "966436/966436 [==============================] - 404s 418us/step\n"
     ]
    }
   ],
   "source": [
    "train_score = lstm_model4.evaluate(x_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy = 0.8201515674591064\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train accuracy = {train_score[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "322145/322145 [==============================] - 132s 410us/step\n"
     ]
    }
   ],
   "source": [
    "val_score = lstm_model4.evaluate(x_val1, y_val1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy = 0.8209471106529236\n"
     ]
    }
   ],
   "source": [
    "print(f\"Validation accuracy = {val_score[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "322147/322147 [==============================] - 123s 382us/step\n"
     ]
    }
   ],
   "source": [
    "test_score = lstm_model4.evaluate(x_test1, y_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy = 0.8212027549743652\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test accuracy = {test_score[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
