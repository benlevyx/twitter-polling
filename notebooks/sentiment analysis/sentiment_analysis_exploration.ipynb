{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense, LSTM, Bidirectional\n",
    "from keras import optimizers\n",
    "from keras import losses\n",
    "from keras import metrics\n",
    "from keras import callbacks\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import io\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that the datasets used to train the following models exceed the limit of 100mb set by github so they have not been uploaded to our online repository. All "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_datasets(proportion=0.5):\n",
    "    data = pd.read_csv('./data/trainingandtestdata/train.csv', encoding='utf-8')\n",
    "    air_reviews = pd.read_csv('./data/twitter-airline-sentiment/Tweets.csv')\n",
    "    \n",
    "    data = data.rename(columns={'0': 'sentiment', '@switchfoot http://twitpic.com/2y1zl - Awww, that\\'s a bummer.  You shoulda got David Carr of Third Day to do it. ;D': 'tweet'})\n",
    "    data_proc = data.sample(frac=proportion, replace=False)\n",
    "    data_proc = data_proc.loc[:, ['tweet', 'sentiment']]\n",
    "    data_proc.loc[data_proc['sentiment'] == 4, 'sentiment'] = 1\n",
    "    \n",
    "    air_reviews = air_reviews.rename(columns={'airline_sentiment': 'sentiment', 'text': 'tweet'})\n",
    "    air_reviews_proc = air_reviews.loc[air_reviews['sentiment'] != 'neutral', ['tweet', 'sentiment']]\n",
    "    air_reviews_proc.loc[air_reviews['sentiment'] == 'positive', 'sentiment'] = 1\n",
    "    air_reviews_proc.loc[air_reviews['sentiment'] == 'negative', 'sentiment'] = 0\n",
    "    \n",
    "    data_concat = pd.concat([data_proc, air_reviews_proc], ignore_index=True)\n",
    "    data_concat = data_concat.sample(frac=1, replace=False).reset_index(drop=True)\n",
    "    return data_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_df = augment_datasets(proportion = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1611540, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow! Congrats @charlene29  very nice</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@dadiaperbank ventral sounds more serious! goo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It's too early</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>THE THING THAT SUCKS MOST ABOUT BRACES IS THAT...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Is sleepy sleepy time...last full day is tomor...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet sentiment\n",
       "0               Wow! Congrats @charlene29  very nice         1\n",
       "1  @dadiaperbank ventral sounds more serious! goo...         0\n",
       "2                                    It's too early          0\n",
       "3  THE THING THAT SUCKS MOST ABOUT BRACES IS THAT...         0\n",
       "4  Is sleepy sleepy time...last full day is tomor...         0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(augmented_df.shape)\n",
    "augmented_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove hashtags, mentions and links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to remove hashtags and mentions\n",
    "def preprocess(tweet):\n",
    "    tweet_words = tweet.split()\n",
    "    for word in tweet_words:\n",
    "        if word.startswith('@') or word.startswith('#') or word.startswith('http'):\n",
    "            tweet_words.remove(word)\n",
    "    return ' '.join(tweet_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_df_proc = augmented_df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_df_proc.iloc[:,0] = aug_df_proc.iloc[:,0].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow! Congrats very nice</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ventral sounds more serious! good luck w/festi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It's too early</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>THE THING THAT SUCKS MOST ABOUT BRACES IS THAT...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Is sleepy sleepy time...last full day is tomorrow</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet sentiment\n",
       "0                            Wow! Congrats very nice         1\n",
       "1  ventral sounds more serious! good luck w/festi...         0\n",
       "2                                     It's too early         0\n",
       "3  THE THING THAT SUCKS MOST ABOUT BRACES IS THAT...         0\n",
       "4  Is sleepy sleepy time...last full day is tomorrow         0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aug_df_proc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and build training, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of words to consider in the dataset\n",
    "max_words = 20000\n",
    "tokenizer = Tokenizer(num_words = 20000)\n",
    "texts = list(aug_df_proc['tweet'].values)\n",
    "#create the token index based on tweets\n",
    "tokenizer.fit_on_texts(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 336434 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "print('Found %s unique tokens.' % len(tokenizer.word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform the tweets to sequences\n",
    "sequences = tokenizer.texts_to_sequences(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the maximum length of each tweet based on dataset\n",
    "lens = [len(x) for x in sequences]\n",
    "max_length = max(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "966924\n",
      "966924\n",
      "322308\n",
      "322308\n",
      "322308\n",
      "322308\n"
     ]
    }
   ],
   "source": [
    "padded_seq = pad_sequences(sequences, maxlen=max_length)\n",
    "labels = augmented_df['sentiment'].values\n",
    "\n",
    "train_proportion = 0.6\n",
    "val_proportion = 0.2\n",
    "\n",
    "\n",
    "x_train = padded_seq[:int(train_proportion*len(padded_seq))]\n",
    "y_train = labels[:int(train_proportion*len(padded_seq))]\n",
    "\n",
    "x_val = padded_seq[int(train_proportion*len(padded_seq)):int(train_proportion*len(padded_seq))+int(val_proportion*len(padded_seq))]\n",
    "y_val = labels[int(train_proportion*len(padded_seq)):int(train_proportion*len(padded_seq))+int(val_proportion*len(padded_seq))]\n",
    "\n",
    "x_test = padded_seq[int(train_proportion*len(padded_seq))+int(val_proportion*len(padded_seq)):]\n",
    "y_test = labels[int(train_proportion*len(padded_seq))+int(val_proportion*len(padded_seq)):]\n",
    "\n",
    "\n",
    "print(len(x_train))\n",
    "print(len(y_train))\n",
    "print(len(x_val))\n",
    "print(len(y_val))\n",
    "print(len(x_test))\n",
    "print(len(y_test))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a baseline neural network with one hidden Dense layer on top of Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/dvamvou/anaconda3/envs/ds/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 100)           2000000   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 5000)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                160032    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,160,065\n",
      "Trainable params: 2,160,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#set the dimensions of the embedding layer, each word now is a vector in\n",
    "#embedding_dim-dimensional space\n",
    "embedding_dim = 100 \n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=max_length))\n",
    "model.add(Flatten())\n",
    "#one hidden layer with 32 neurons\n",
    "model.add(Dense(32, activation='relu'))\n",
    "#output layer\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/dvamvou/anaconda3/envs/ds/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 966924 samples, validate on 322308 samples\n",
      "Epoch 1/10\n",
      "966924/966924 [==============================] - 658s 681us/step - loss: 0.4710 - acc: 0.7819 - val_loss: 0.4679 - val_acc: 0.7786\n",
      "Epoch 2/10\n",
      "966924/966924 [==============================] - 650s 673us/step - loss: 0.4694 - acc: 0.7897 - val_loss: 0.4863 - val_acc: 0.7840\n",
      "Epoch 3/10\n",
      "966924/966924 [==============================] - 694s 718us/step - loss: 0.4771 - acc: 0.7916 - val_loss: 0.4949 - val_acc: 0.7915\n",
      "Epoch 4/10\n",
      "966924/966924 [==============================] - 627s 648us/step - loss: 0.4840 - acc: 0.7917 - val_loss: 0.5100 - val_acc: 0.7826\n",
      "Epoch 5/10\n",
      "966924/966924 [==============================] - 680s 703us/step - loss: 0.4918 - acc: 0.7919 - val_loss: 0.5091 - val_acc: 0.7892\n",
      "Epoch 6/10\n",
      "966924/966924 [==============================] - 641s 663us/step - loss: 0.5040 - acc: 0.7914 - val_loss: 0.5322 - val_acc: 0.7900\n",
      "Epoch 7/10\n",
      "966924/966924 [==============================] - 565s 584us/step - loss: 0.5136 - acc: 0.7910 - val_loss: 0.5030 - val_acc: 0.7909\n",
      "Epoch 8/10\n",
      "966924/966924 [==============================] - 594s 614us/step - loss: 0.5321 - acc: 0.7907 - val_loss: 0.5857 - val_acc: 0.7856\n",
      "Epoch 9/10\n",
      "966924/966924 [==============================] - 646s 668us/step - loss: 0.5596 - acc: 0.7906 - val_loss: 0.6281 - val_acc: 0.7846\n",
      "Epoch 10/10\n",
      "966924/966924 [==============================] - 634s 656us/step - loss: 0.5918 - acc: 0.7906 - val_loss: 0.6295 - val_acc: 0.7908\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('simple_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "322308/322308 [==============================] - 8s 24us/step\n",
      "Model 1 accuracy on test set: 0.7900145053863525\n"
     ]
    }
   ],
   "source": [
    "print(f'Model 1 accuracy on test set: {model.evaluate(x_test, y_test)[1]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use LSTM hidden layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_LSTM1 = [\n",
    "        callbacks.EarlyStopping(\n",
    "            monitor='acc',\n",
    "            patience=1, #stop training if accuracy has not improved for 2 epochs\n",
    "        ),\n",
    "        callbacks.ModelCheckpoint(\n",
    "            filepath='LSTM_model1.h5',\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "        )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 50, 100)           2000000   \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 50, 64)            42240     \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 32)                12416     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,055,745\n",
      "Trainable params: 2,055,745\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 100 \n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(Embedding(max_words, embedding_dim, input_length=max_length))\n",
    "lstm_model.add(LSTM(64, return_sequences=True))\n",
    "lstm_model.add(LSTM(32))\n",
    "lstm_model.add(Dense(32, activation='relu'))\n",
    "#output layer\n",
    "lstm_model.add(Dense(1, activation='sigmoid'))\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 966924 samples, validate on 322308 samples\n",
      "Epoch 1/10\n",
      "966924/966924 [==============================] - 2085s 2ms/step - loss: 0.4240 - acc: 0.8057 - val_loss: 0.4014 - val_acc: 0.8193\n",
      "Epoch 2/10\n",
      "966924/966924 [==============================] - 1967s 2ms/step - loss: 0.3944 - acc: 0.8240 - val_loss: 0.3954 - val_acc: 0.8230\n",
      "Epoch 3/10\n",
      "966924/966924 [==============================] - 2093s 2ms/step - loss: 0.3879 - acc: 0.8285 - val_loss: 0.3967 - val_acc: 0.8241\n",
      "Epoch 4/10\n",
      "966924/966924 [==============================] - 2287s 2ms/step - loss: 0.3833 - acc: 0.8312 - val_loss: 0.3927 - val_acc: 0.8253\n",
      "Epoch 5/10\n",
      "966924/966924 [==============================] - 2355s 2ms/step - loss: 0.3809 - acc: 0.8332 - val_loss: 0.3942 - val_acc: 0.8253\n",
      "Epoch 6/10\n",
      "966924/966924 [==============================] - 2037s 2ms/step - loss: 0.3786 - acc: 0.8348 - val_loss: 0.4000 - val_acc: 0.8233\n",
      "Epoch 7/10\n",
      "966924/966924 [==============================] - 1791s 2ms/step - loss: 0.3766 - acc: 0.8367 - val_loss: 0.4008 - val_acc: 0.8239\n",
      "Epoch 8/10\n",
      "966924/966924 [==============================] - 1787s 2ms/step - loss: 0.3750 - acc: 0.8380 - val_loss: 0.3979 - val_acc: 0.8231\n",
      "Epoch 9/10\n",
      "966924/966924 [==============================] - 1793s 2ms/step - loss: 0.3737 - acc: 0.8388 - val_loss: 0.4070 - val_acc: 0.8232\n",
      "Epoch 10/10\n",
      "966924/966924 [==============================] - 1791s 2ms/step - loss: 0.3746 - acc: 0.8392 - val_loss: 0.4060 - val_acc: 0.8224\n"
     ]
    }
   ],
   "source": [
    "lstm_model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "history = lstm_model.fit(x_train, y_train, \n",
    "                    epochs=10, \n",
    "                    batch_size=32, \n",
    "                    callbacks=callbacks_LSTM1,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "322308/322308 [==============================] - 254s 788us/step\n",
      "Model 1 accuracy on test set: 0.8211400508880615\n"
     ]
    }
   ],
   "source": [
    "print(f'Model 1 accuracy on test set: {lstm_model.evaluate(x_test, y_test)[1]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Bidirectional LSTM layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_LSTM2 = [\n",
    "        callbacks.EarlyStopping(\n",
    "            monitor='acc',\n",
    "            patience=1, #stop training if accuracy has not improved for 2 epochs\n",
    "        ),\n",
    "        callbacks.ModelCheckpoint(\n",
    "            filepath='LSTM_model2.h5',\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "        )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_13 (Embedding)     (None, 50, 100)           2000000   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 50, 128)           84480     \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 64)                41216     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,127,809\n",
      "Trainable params: 2,127,809\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 100 \n",
    "lstm_model2 = Sequential()\n",
    "lstm_model2.add(Embedding(max_words, embedding_dim, input_length=max_length))\n",
    "lstm_model2.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "lstm_model2.add(Bidirectional(LSTM(32)))\n",
    "lstm_model2.add(Dense(32, activation='relu'))\n",
    "#output layer\n",
    "lstm_model2.add(Dense(1, activation='sigmoid'))\n",
    "lstm_model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 966924 samples, validate on 322308 samples\n",
      "Epoch 1/10\n",
      "966924/966924 [==============================] - 3954s 4ms/step - loss: 0.4259 - acc: 0.8045 - val_loss: 0.4056 - val_acc: 0.8166\n",
      "Epoch 2/10\n",
      "966924/966924 [==============================] - 3308s 3ms/step - loss: 0.3963 - acc: 0.8228 - val_loss: 0.3976 - val_acc: 0.8224\n",
      "Epoch 3/10\n",
      "966924/966924 [==============================] - 3040s 3ms/step - loss: 0.3892 - acc: 0.8276 - val_loss: 0.3922 - val_acc: 0.8232\n",
      "Epoch 4/10\n",
      "966924/966924 [==============================] - 3371s 3ms/step - loss: 0.3849 - acc: 0.8305 - val_loss: 0.3945 - val_acc: 0.8250\n",
      "Epoch 5/10\n",
      "966924/966924 [==============================] - 3280s 3ms/step - loss: 0.3821 - acc: 0.8324 - val_loss: 0.3917 - val_acc: 0.8251\n",
      "Epoch 6/10\n",
      "966924/966924 [==============================] - 3175s 3ms/step - loss: 0.3792 - acc: 0.8342 - val_loss: 0.3916 - val_acc: 0.8246\n",
      "Epoch 7/10\n",
      "966924/966924 [==============================] - 2787s 3ms/step - loss: 0.3781 - acc: 0.8351 - val_loss: 0.3987 - val_acc: 0.8224\n",
      "Epoch 8/10\n",
      "966924/966924 [==============================] - 2762s 3ms/step - loss: 0.3773 - acc: 0.8360 - val_loss: 0.3970 - val_acc: 0.8222\n",
      "Epoch 9/10\n",
      "966924/966924 [==============================] - 2763s 3ms/step - loss: 0.3755 - acc: 0.8369 - val_loss: 0.3954 - val_acc: 0.8226\n",
      "Epoch 10/10\n",
      "966924/966924 [==============================] - 2760s 3ms/step - loss: 0.3744 - acc: 0.8379 - val_loss: 0.3973 - val_acc: 0.8236\n"
     ]
    }
   ],
   "source": [
    "lstm_model2.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "history = lstm_model2.fit(x_train, y_train, \n",
    "                    epochs=10, \n",
    "                    batch_size=32, \n",
    "                    callbacks=callbacks_LSTM2,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Pretrained GloVe Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1193514 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open('./glove.twitter.27B/glove.twitter.27B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 200000\n",
    "word_index = tokenizer.word_index\n",
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i < max_words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 50, 100)           10000000  \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 50, 64)            42240     \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 32)                12416     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 80)                2640      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 81        \n",
      "=================================================================\n",
      "Total params: 10,057,377\n",
      "Trainable params: 10,057,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 966924 samples, validate on 322308 samples\n",
      "Epoch 1/10\n",
      "966924/966924 [==============================] - 1504s 2ms/step - loss: 0.4293 - acc: 0.7996 - val_loss: 0.4061 - val_acc: 0.8136\n",
      "Epoch 2/10\n",
      "966924/966924 [==============================] - 1347s 1ms/step - loss: 0.3949 - acc: 0.8198 - val_loss: 0.3941 - val_acc: 0.8199\n",
      "Epoch 3/10\n",
      "966924/966924 [==============================] - 1326s 1ms/step - loss: 0.3825 - acc: 0.8269 - val_loss: 0.3919 - val_acc: 0.8218\n",
      "Epoch 4/10\n",
      "966924/966924 [==============================] - 1360s 1ms/step - loss: 0.3742 - acc: 0.8314 - val_loss: 0.3900 - val_acc: 0.8240\n",
      "Epoch 5/10\n",
      "966924/966924 [==============================] - 1557s 2ms/step - loss: 0.3682 - acc: 0.8345 - val_loss: 0.3881 - val_acc: 0.8240\n",
      "Epoch 6/10\n",
      "966924/966924 [==============================] - 1478s 2ms/step - loss: 0.3631 - acc: 0.8371 - val_loss: 0.3904 - val_acc: 0.8217\n",
      "Epoch 7/10\n",
      "966924/966924 [==============================] - 1395s 1ms/step - loss: 0.3590 - acc: 0.8393 - val_loss: 0.3896 - val_acc: 0.8231\n",
      "Epoch 8/10\n",
      "966924/966924 [==============================] - 1561s 2ms/step - loss: 0.3553 - acc: 0.8411 - val_loss: 0.3927 - val_acc: 0.8218\n",
      "Epoch 9/10\n",
      "966924/966924 [==============================] - 1425s 1ms/step - loss: 0.3520 - acc: 0.8431 - val_loss: 0.3932 - val_acc: 0.8226\n",
      "Epoch 10/10\n",
      "966924/966924 [==============================] - 1450s 1ms/step - loss: 0.3492 - acc: 0.8441 - val_loss: 0.3949 - val_acc: 0.8210\n"
     ]
    }
   ],
   "source": [
    "callbacks_LSTM_pre = [\n",
    "        callbacks.EarlyStopping(\n",
    "            monitor='acc',\n",
    "            patience=1, #stop training if accuracy has not improved for 2 epochs\n",
    "        ),\n",
    "        callbacks.ModelCheckpoint(\n",
    "            filepath='LSTM_model3_pre.h5',\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "        )\n",
    "]\n",
    "\n",
    "lstm_model3 = Sequential()\n",
    "lstm_model3.add(Embedding(max_words, embedding_dim, input_length=max_length))\n",
    "lstm_model3.add(LSTM(64, return_sequences=True))\n",
    "lstm_model3.add(LSTM(32))\n",
    "lstm_model3.add(Dense(80, activation='relu'))\n",
    "#output layer\n",
    "lstm_model3.add(Dense(1, activation='sigmoid'))\n",
    "lstm_model3.summary()\n",
    "\n",
    "lstm_model3.layers[0].set_weights([embedding_matrix])\n",
    "lstm_model3.layers[0].trainable = False\n",
    "\n",
    "lstm_model3.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "history = lstm_model3.fit(x_train, y_train, \n",
    "                    epochs=10, \n",
    "                    batch_size=32, \n",
    "                    callbacks=callbacks_LSTM_pre,\n",
    "                    validation_data=(x_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "322308/322308 [==============================] - 141s 437us/step\n",
      "Model 3 accuracy on test set: 0.8208855986595154\n"
     ]
    }
   ],
   "source": [
    "print(f'Model 3 accuracy on test set: {lstm_model3.evaluate(x_test, y_test)[1]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6185163 ],\n",
       "       [0.41497543],\n",
       "       [0.44421557],\n",
       "       [0.869727  ],\n",
       "       [0.2883256 ],\n",
       "       [0.8102244 ],\n",
       "       [0.79652214],\n",
       "       [0.71278673],\n",
       "       [0.9978855 ],\n",
       "       [0.3832898 ]], dtype=float32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "political_tweets = [\"You will come in 4th in New Hampshire in February and withdraw in shame. But don't worry... you can use Waze to get back to Massachusetts. You remember Mass., don't you\",\n",
    "                   \"You Can NEVER EARN My Vote ! \",\n",
    "                   \"’ll donate 0. Go home.\",\n",
    "                   \"You are not a true progressive.\",\n",
    "                   \"That’s like throwing cash on the floor and pissing on it!\",\n",
    "                   \"'m a continuing contributor and have been since the day she announced. Every Plan that comes out: showing how it is needed, how it is paid for and how it will increase economic growth in a Green Economy - makes me proud to support her all the way to the White House!\",\n",
    "                   \"Your toes? Yuck, no thanks\",\n",
    "                   \"God bless him . @PeteButtigieg by far the most honest and innovative candidate with real solutions to our Problems  !!!!\",\n",
    "                   \"Go Warren I'm proud of you\",\n",
    "                   \"You are bad\"]\n",
    "political_tweets_proc = list(map(preprocess, political_tweets))\n",
    "pol_seqs = tokenizer.texts_to_sequences(political_tweets_proc)\n",
    "pol_seqs_padded = pad_sequences(pol_seqs, maxlen=max_length)\n",
    "lstm_model3.predict(pol_seqs_padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add some more related labelled data: \n",
    "### Tweets from the GOP debate in 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_datasets2(proportion=0.5):\n",
    "    data = pd.read_csv('./data/trainingandtestdata/train.csv', encoding='utf-8')\n",
    "    gop_debate = pd.read_csv(\"./data/trainingandtestdata/Sentiment.csv\")\n",
    "    \n",
    "    data = data.rename(columns={'0': 'sentiment', '@switchfoot http://twitpic.com/2y1zl - Awww, that\\'s a bummer.  You shoulda got David Carr of Third Day to do it. ;D': 'tweet'})\n",
    "    data_proc = data.sample(frac=proportion, replace=False)\n",
    "    data_proc = data_proc.loc[:, ['tweet', 'sentiment']]\n",
    "    data_proc.loc[data_proc['sentiment'] == 4, 'sentiment'] = 1\n",
    "    \n",
    "    gop_debate = gop_debate.rename(columns={'text': 'tweet'})\n",
    "    gop_debate_proc = gop_debate.loc[gop_debate['sentiment'] != 'Neutral', ['tweet', 'sentiment']]\n",
    "    gop_debate_proc.loc[gop_debate_proc['sentiment'] == 'Positive', 'sentiment'] = 1\n",
    "    gop_debate_proc.loc[gop_debate_proc['sentiment'] == 'Negative', 'sentiment'] = 0\n",
    "    \n",
    "    data_concat = pd.concat([data_proc, gop_debate_proc], ignore_index=True)\n",
    "    data_concat = data_concat.sample(frac=1, replace=False).reset_index(drop=True)\n",
    "    return data_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_df2 = augment_datasets2(proportion = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1610728, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>is watchin friends</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I can't stand this heat  roll on winter</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@mariancall I'm just glad that you didn't thin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Leelian972 Its not found yet n its extremely ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Misses matty poo!  he'll get a dutch rudder wh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet sentiment\n",
       "0                                is watchin friends          1\n",
       "1            I can't stand this heat  roll on winter         0\n",
       "2  @mariancall I'm just glad that you didn't thin...         1\n",
       "3  @Leelian972 Its not found yet n its extremely ...         0\n",
       "4  Misses matty poo!  he'll get a dutch rudder wh...         0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(augmented_df2.shape)\n",
    "augmented_df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove stopwords, emoticons, hashtags and mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'over',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'too',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'should',\n",
       " 'now']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = []\n",
    "with open(\"./data/stopwords.txt\") as f:\n",
    "    lines = f.readlines()\n",
    "for i in range(1,len(lines)):\n",
    "    stopwords.append(lines[i].strip())\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[':-@',\n",
       " '>:o',\n",
       " '>:0',\n",
       " 'D:<',\n",
       " 'D:',\n",
       " 'D8',\n",
       " 'D;',\n",
       " 'D=',\n",
       " 'Dx',\n",
       " '>.<',\n",
       " '>_<',\n",
       " 'd:<',\n",
       " 'd:',\n",
       " 'd8',\n",
       " 'd;',\n",
       " 'd=',\n",
       " 'dx',\n",
       " 'v.v',\n",
       " ':/',\n",
       " ':\\\\',\n",
       " '=/',\n",
       " '=\\\\',\n",
       " '>:/',\n",
       " '>:\\\\',\n",
       " ':-/',\n",
       " ':-\\\\',\n",
       " ':)',\n",
       " '(:',\n",
       " ';)',\n",
       " ';(',\n",
       " '(;',\n",
       " ');',\n",
       " ':-)',\n",
       " ':3',\n",
       " ':d',\n",
       " ':D',\n",
       " 'xd',\n",
       " \":')\",\n",
       " '^_^',\n",
       " '^.^',\n",
       " ':]',\n",
       " ':}',\n",
       " ':p',\n",
       " ':b',\n",
       " '=p',\n",
       " '=b',\n",
       " ':-p',\n",
       " ':-b',\n",
       " '=)',\n",
       " ':(',\n",
       " '):',\n",
       " \":'(\",\n",
       " ':c',\n",
       " ':-(',\n",
       " '</3',\n",
       " ':[',\n",
       " ':{',\n",
       " 'T.T',\n",
       " 'o_o',\n",
       " 'O_O',\n",
       " '0_o',\n",
       " 'o_0',\n",
       " '0_O',\n",
       " 'O_0',\n",
       " 'o.o',\n",
       " 'O.O',\n",
       " '0.o',\n",
       " 'o.0',\n",
       " ':o',\n",
       " ':-o',\n",
       " '<3',\n",
       " ':p',\n",
       " ':b',\n",
       " '=p',\n",
       " '=b',\n",
       " ':-p',\n",
       " ':-b',\n",
       " ':$']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoticons = []\n",
    "with open(\"./data/emoticons.txt\") as f:\n",
    "    lines = f.readlines()\n",
    "for i in range(1,len(lines)):\n",
    "    emoticons.append(lines[i].strip())\n",
    "emoticons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to remove hashtags and mentions\n",
    "#remove stopwords and emoticons\n",
    "#trasform everything to lowercase\n",
    "def preprocess_tweet(tweet):\n",
    "    tweet_lower = tweet.lower()\n",
    "    tweet_words = tweet_lower.split()\n",
    "    toberemoved = []\n",
    "    for word in tweet_words:\n",
    "        if word.startswith('@') or word.startswith('#') or word.startswith('http'):\n",
    "            toberemoved.append(word)\n",
    "        elif word in stopwords or word in emoticons:\n",
    "            toberemoved.append(word)\n",
    "    for word in toberemoved:\n",
    "        tweet_words.remove(word)\n",
    "\n",
    "    return ' '.join(tweet_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i kno! u would quit disappearin!!!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>danny i one follower cause i don't let anyone ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lmfao. ily guys know. i live ontario. it's bor...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>alright? x</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>really never ended special</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet sentiment\n",
       "0                 i kno! u would quit disappearin!!!         1\n",
       "1  danny i one follower cause i don't let anyone ...         1\n",
       "2  lmfao. ily guys know. i live ontario. it's bor...         0\n",
       "3                                         alright? x         0\n",
       "4                         really never ended special         0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_df2.iloc[:,0] = augmented_df2.iloc[:,0].map(preprocess_tweet)\n",
    "augmented_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    808492\n",
       "1    802236\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_df2['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of words to consider in the dataset\n",
    "max_words = 300000\n",
    "tokenizer = Tokenizer(num_words = max_words)\n",
    "texts = list(augmented_df2['tweet'].values)\n",
    "#create the token index based on tweets\n",
    "tokenizer.fit_on_texts(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 312520 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "print('Found %s unique tokens.' % len(tokenizer.word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "966436\n",
      "966436\n",
      "322145\n",
      "322145\n",
      "322147\n",
      "322147\n"
     ]
    }
   ],
   "source": [
    "sequences1 = tokenizer.texts_to_sequences(texts)\n",
    "#set the maximum length of each tweet based on dataset\n",
    "lens1 = [len(x) for x in sequences1]\n",
    "max_length1 = max(lens1)\n",
    "\n",
    "padded_seq1 = pad_sequences(sequences1, maxlen=max_length1)\n",
    "labels1 = augmented_df2['sentiment'].values\n",
    "\n",
    "train_proportion = 0.6\n",
    "val_proportion = 0.2\n",
    "\n",
    "\n",
    "x_train1 = padded_seq1[:int(train_proportion*len(padded_seq1))]\n",
    "y_train1 = labels1[:int(train_proportion*len(padded_seq1))]\n",
    "\n",
    "x_val1 = padded_seq1[int(train_proportion*len(padded_seq1)):int(train_proportion*len(padded_seq1))+int(val_proportion*len(padded_seq1))]\n",
    "y_val1 = labels1[int(train_proportion*len(padded_seq1)):int(train_proportion*len(padded_seq1))+int(val_proportion*len(padded_seq1))]\n",
    "\n",
    "x_test1 = padded_seq1[int(train_proportion*len(padded_seq1))+int(val_proportion*len(padded_seq1)):]\n",
    "y_test1 = labels1[int(train_proportion*len(padded_seq1))+int(val_proportion*len(padded_seq1)):]\n",
    "\n",
    "\n",
    "print(len(x_train1))\n",
    "print(len(y_train1))\n",
    "print(len(x_val1))\n",
    "print(len(y_val1))\n",
    "print(len(x_test1))\n",
    "print(len(y_test1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 50, 100)           30000000  \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 50, 64)            42240     \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 32)                12416     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 30,055,745\n",
      "Trainable params: 30,055,745\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 966436 samples, validate on 322145 samples\n",
      "Epoch 1/10\n",
      "966436/966436 [==============================] - 1526s 2ms/step - loss: 0.4418 - acc: 0.7921 - val_loss: 0.4212 - val_acc: 0.8046\n",
      "Epoch 2/10\n",
      "966436/966436 [==============================] - 1368s 1ms/step - loss: 0.4125 - acc: 0.8100 - val_loss: 0.4169 - val_acc: 0.8067\n",
      "Epoch 3/10\n",
      "966436/966436 [==============================] - 1322s 1ms/step - loss: 0.4015 - acc: 0.8157 - val_loss: 0.4098 - val_acc: 0.8115\n",
      "Epoch 4/10\n",
      "966436/966436 [==============================] - 1323s 1ms/step - loss: 0.3942 - acc: 0.8198 - val_loss: 0.4084 - val_acc: 0.8120\n",
      "Epoch 5/10\n",
      "966436/966436 [==============================] - 1324s 1ms/step - loss: 0.3885 - acc: 0.8234 - val_loss: 0.4097 - val_acc: 0.8124\n",
      "Epoch 6/10\n",
      "966436/966436 [==============================] - 1322s 1ms/step - loss: 0.3837 - acc: 0.8258 - val_loss: 0.4117 - val_acc: 0.8111\n",
      "Epoch 7/10\n",
      "966436/966436 [==============================] - 1324s 1ms/step - loss: 0.3795 - acc: 0.8278 - val_loss: 0.4136 - val_acc: 0.8109\n",
      "Epoch 8/10\n",
      "966436/966436 [==============================] - 1323s 1ms/step - loss: 0.3760 - acc: 0.8298 - val_loss: 0.4122 - val_acc: 0.8103\n",
      "Epoch 9/10\n",
      "966436/966436 [==============================] - 1323s 1ms/step - loss: 0.3728 - acc: 0.8314 - val_loss: 0.4154 - val_acc: 0.8095\n",
      "Epoch 10/10\n",
      "966436/966436 [==============================] - 1321s 1ms/step - loss: 0.3698 - acc: 0.8334 - val_loss: 0.4152 - val_acc: 0.8094\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i < max_words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "            \n",
    "callbacks_LSTM_pre2 = [\n",
    "        callbacks.EarlyStopping(\n",
    "            monitor='acc',\n",
    "            patience=1, #stop training if accuracy has not improved for 2 epochs\n",
    "        ),\n",
    "        callbacks.ModelCheckpoint(\n",
    "            filepath='LSTM_model4_nostop.h5',\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "        )\n",
    "]\n",
    "\n",
    "lstm_model4 = Sequential()\n",
    "lstm_model4.add(Embedding(max_words, embedding_dim, input_length=max_length))\n",
    "lstm_model4.add(LSTM(64, return_sequences=True))\n",
    "lstm_model4.add(LSTM(32))\n",
    "lstm_model4.add(Dense(32, activation='relu'))\n",
    "#output layer\n",
    "lstm_model4.add(Dense(1, activation='sigmoid'))\n",
    "lstm_model4.summary()\n",
    "\n",
    "lstm_model4.layers[0].set_weights([embedding_matrix])\n",
    "lstm_model4.layers[0].trainable = False\n",
    "\n",
    "lstm_model4.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "history = lstm_model4.fit(x_train1, y_train1, \n",
    "                    epochs=10, \n",
    "                    batch_size=32, \n",
    "                    callbacks=callbacks_LSTM_pre2,\n",
    "                    validation_data=(x_val1, y_val1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the behaviour of the classifier on some custom data which resemble the political tweets that we want to predict the sentiment of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.3400808 ],\n",
       "       [0.03073052],\n",
       "       [0.6530683 ],\n",
       "       [0.3990408 ],\n",
       "       [0.63873905],\n",
       "       [0.9362867 ],\n",
       "       [0.21128505],\n",
       "       [0.68271136],\n",
       "       [0.9712548 ],\n",
       "       [0.12916476],\n",
       "       [0.8177171 ],\n",
       "       [0.29179707],\n",
       "       [0.02000201],\n",
       "       [0.92986953],\n",
       "       [0.7166861 ],\n",
       "       [0.10063127],\n",
       "       [0.12219265],\n",
       "       [0.9922502 ],\n",
       "       [0.93887365],\n",
       "       [0.6897486 ],\n",
       "       [0.93887365],\n",
       "       [0.3357073 ],\n",
       "       [0.5074039 ],\n",
       "       [0.9185116 ]], dtype=float32)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "political_tweets = [\"You will come in 4th in New Hampshire in February and withdraw in shame. But don't worry... you can use Waze to get back to Massachusetts. You remember Mass., don't you\",\n",
    "                   \"You Can NEVER EARN My Vote ! \",\n",
    "                   \"’ll donate 0. Go home.\",\n",
    "                   \"You are not a true progressive.\",\n",
    "                   \"That’s like throwing cash on the floor and pissing on it!\",\n",
    "                   \"'m a continuing contributor and have been since the day she announced. Every Plan that comes out: showing how it is needed, how it is paid for and how it will increase economic growth in a Green Economy - makes me proud to support her all the way to the White House!\",\n",
    "                   \"Your toes? Yuck, no thanks\",\n",
    "                   \"God bless him . @PeteButtigieg by far the most honest and innovative candidate with real solutions to our Problems  !!!!\",\n",
    "                   \"Go Warren I'm proud of you\",\n",
    "                   \"You are bad\",\n",
    "                   \"you are a true fighter\",\n",
    "                   \"you are a liar!\",\n",
    "                   \"I do not support you\",\n",
    "                   \"I support you!\",\n",
    "                   \"Get out there and reach the people!! Out of all the democratic candidates I believe that your message can resonate with everyone. I trust you. Your logic and calm level headed approach reminds me of how politics should be. I want you to succeed.\",\n",
    "                   \"It’s not your time Pete. Drop out and run for Governor you’re doing more harm to the party than good.\",\n",
    "                    \"Hell No!\",\n",
    "                    \"You're amazing. You're qualified, and you're ready\",\n",
    "                    \"by far the most honest and innovative candidate with real solutions\",\n",
    "                    \"HE IS PROGRESSIVE\",\n",
    "                    \"by far the most honest and innovative candidate with real solutions\",\n",
    "                    \"Yeah it's called lying she does it very well if you don't remember correctly last year she was a native American off with her head she is the scum of the Earth\",\n",
    "                    \"0 votes for you during those elections\"\n",
    "                    \"Yes, somehow you war criminals manage to get away with it every time.\",\n",
    "                    \"Get 'em Joe.\"\n",
    "                   ]\n",
    "political_tweets_proc = list(map(preprocess_tweet, political_tweets))\n",
    "pol_seqs = tokenizer.texts_to_sequences(political_tweets_proc)\n",
    "pol_seqs_padded = pad_sequences(pol_seqs, maxlen=max_length)\n",
    "lstm_model4.predict(pol_seqs_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "322147/322147 [==============================] - 173s 537us/step\n",
      "Model 4 accuracy on test set: 0.8087487816810608\n"
     ]
    }
   ],
   "source": [
    "print(f'Model 4 accuracy on test set: {lstm_model4.evaluate(x_test1, y_test1)[1]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_json = tokenizer.to_json()\n",
    "with io.open('tokenizer.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(tokenizer_json, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce the model parameters by decreasing the max_words of the tokenizer to 200k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of words to consider in the dataset\n",
    "max_words = 200000\n",
    "tokenizer = Tokenizer(num_words = max_words)\n",
    "texts = list(augmented_df2['tweet'].values)\n",
    "#create the token index based on tweets\n",
    "tokenizer.fit_on_texts(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "966436\n",
      "966436\n",
      "322145\n",
      "322145\n",
      "322147\n",
      "322147\n"
     ]
    }
   ],
   "source": [
    "sequences1 = tokenizer.texts_to_sequences(texts)\n",
    "#set the maximum length of each tweet based on dataset\n",
    "lens1 = [len(x) for x in sequences1]\n",
    "max_length1 = max(lens1)\n",
    "\n",
    "padded_seq1 = pad_sequences(sequences1, maxlen=max_length1)\n",
    "labels1 = augmented_df2['sentiment'].values\n",
    "\n",
    "train_proportion = 0.6\n",
    "val_proportion = 0.2\n",
    "\n",
    "\n",
    "x_train1 = padded_seq1[:int(train_proportion*len(padded_seq1))]\n",
    "y_train1 = labels1[:int(train_proportion*len(padded_seq1))]\n",
    "\n",
    "x_val1 = padded_seq1[int(train_proportion*len(padded_seq1)):int(train_proportion*len(padded_seq1))+int(val_proportion*len(padded_seq1))]\n",
    "y_val1 = labels1[int(train_proportion*len(padded_seq1)):int(train_proportion*len(padded_seq1))+int(val_proportion*len(padded_seq1))]\n",
    "\n",
    "x_test1 = padded_seq1[int(train_proportion*len(padded_seq1))+int(val_proportion*len(padded_seq1)):]\n",
    "y_test1 = labels1[int(train_proportion*len(padded_seq1))+int(val_proportion*len(padded_seq1)):]\n",
    "\n",
    "\n",
    "print(len(x_train1))\n",
    "print(len(y_train1))\n",
    "print(len(x_val1))\n",
    "print(len(y_val1))\n",
    "print(len(x_test1))\n",
    "print(len(y_test1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a unidirectional LSTM netwotk with pretrained embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/dvamvou/anaconda3/envs/ds/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 100)           20000000  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50, 64)            42240     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 32)                12416     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 20,055,745\n",
      "Trainable params: 20,055,745\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /Users/dvamvou/anaconda3/envs/ds/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 966436 samples, validate on 322145 samples\n",
      "Epoch 1/10\n",
      "966436/966436 [==============================] - 1558s 2ms/step - loss: 0.4421 - acc: 0.7919 - val_loss: 0.4225 - val_acc: 0.8041\n",
      "Epoch 2/10\n",
      "966436/966436 [==============================] - 1669s 2ms/step - loss: 0.4132 - acc: 0.8093 - val_loss: 0.4146 - val_acc: 0.8087\n",
      "Epoch 3/10\n",
      "966436/966436 [==============================] - 1415s 1ms/step - loss: 0.4020 - acc: 0.8155 - val_loss: 0.4114 - val_acc: 0.8103\n",
      "Epoch 4/10\n",
      "966436/966436 [==============================] - 1424s 1ms/step - loss: 0.3946 - acc: 0.8196 - val_loss: 0.4108 - val_acc: 0.8109\n",
      "Epoch 5/10\n",
      "966436/966436 [==============================] - 1423s 1ms/step - loss: 0.3888 - acc: 0.8229 - val_loss: 0.4117 - val_acc: 0.8105\n",
      "Epoch 6/10\n",
      "966436/966436 [==============================] - 1415s 1ms/step - loss: 0.3839 - acc: 0.8254 - val_loss: 0.4139 - val_acc: 0.8101\n",
      "Epoch 7/10\n",
      "966436/966436 [==============================] - 1315s 1ms/step - loss: 0.3799 - acc: 0.8275 - val_loss: 0.4135 - val_acc: 0.8098\n",
      "Epoch 8/10\n",
      "966436/966436 [==============================] - 1314s 1ms/step - loss: 0.3763 - acc: 0.8293 - val_loss: 0.4160 - val_acc: 0.8079\n",
      "Epoch 9/10\n",
      "966436/966436 [==============================] - 1311s 1ms/step - loss: 0.3729 - acc: 0.8315 - val_loss: 0.4192 - val_acc: 0.8085\n",
      "Epoch 10/10\n",
      "966436/966436 [==============================] - 1313s 1ms/step - loss: 0.3698 - acc: 0.8330 - val_loss: 0.4252 - val_acc: 0.8083\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i < max_words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "            \n",
    "callbacks_LSTM_pre3 = [\n",
    "        callbacks.EarlyStopping(\n",
    "            monitor='acc',\n",
    "            patience=1, #stop training if accuracy has not improved for 2 epochs\n",
    "        ),\n",
    "        callbacks.ModelCheckpoint(\n",
    "            filepath='LSTM_model5_nostop.h5',\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "        )\n",
    "]\n",
    "\n",
    "lstm_model5 = Sequential()\n",
    "lstm_model5.add(Embedding(max_words, embedding_dim, input_length=max_length1))\n",
    "lstm_model5.add(LSTM(64, return_sequences=True))\n",
    "lstm_model5.add(LSTM(32))\n",
    "lstm_model5.add(Dense(32, activation='relu'))\n",
    "#output layer\n",
    "lstm_model5.add(Dense(1, activation='sigmoid'))\n",
    "lstm_model5.summary()\n",
    "\n",
    "lstm_model5.layers[0].set_weights([embedding_matrix])\n",
    "lstm_model5.layers[0].trainable = False\n",
    "\n",
    "lstm_model5.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "history = lstm_model5.fit(x_train1, y_train1, \n",
    "                    epochs=10, \n",
    "                    batch_size=32, \n",
    "                    callbacks=callbacks_LSTM_pre3,\n",
    "                    validation_data=(x_val1, y_val1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.36327988],\n",
       "       [0.01246384],\n",
       "       [0.6152877 ],\n",
       "       [0.38871288],\n",
       "       [0.10964805],\n",
       "       [0.92565966],\n",
       "       [0.08628792],\n",
       "       [0.9773206 ],\n",
       "       [0.99119425],\n",
       "       [0.14640832],\n",
       "       [0.92075986],\n",
       "       [0.3329754 ],\n",
       "       [0.07705918],\n",
       "       [0.986609  ],\n",
       "       [0.8022307 ],\n",
       "       [0.43946102],\n",
       "       [0.14107105],\n",
       "       [0.96037275],\n",
       "       [0.9534604 ],\n",
       "       [0.86061513],\n",
       "       [0.9534604 ],\n",
       "       [0.51057696],\n",
       "       [0.35316393],\n",
       "       [0.8803505 ]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "political_tweets = [\"You will come in 4th in New Hampshire in February and withdraw in shame. But don't worry... you can use Waze to get back to Massachusetts. You remember Mass., don't you\",\n",
    "                   \"You Can NEVER EARN My Vote ! \",\n",
    "                   \"’ll donate 0. Go home.\",\n",
    "                   \"You are not a true progressive.\",\n",
    "                   \"That’s like throwing cash on the floor and pissing on it!\",\n",
    "                   \"'m a continuing contributor and have been since the day she announced. Every Plan that comes out: showing how it is needed, how it is paid for and how it will increase economic growth in a Green Economy - makes me proud to support her all the way to the White House!\",\n",
    "                   \"Your toes? Yuck, no thanks\",\n",
    "                   \"God bless him . @PeteButtigieg by far the most honest and innovative candidate with real solutions to our Problems  !!!!\",\n",
    "                   \"Go Warren I'm proud of you\",\n",
    "                   \"You are bad\",\n",
    "                   \"you are a true fighter\",\n",
    "                   \"you are a liar!\",\n",
    "                   \"I do not support you\",\n",
    "                   \"I support you!\",\n",
    "                   \"Get out there and reach the people!! Out of all the democratic candidates I believe that your message can resonate with everyone. I trust you. Your logic and calm level headed approach reminds me of how politics should be. I want you to succeed.\",\n",
    "                   \"It’s not your time Pete. Drop out and run for Governor you’re doing more harm to the party than good.\",\n",
    "                    \"Hell No!\",\n",
    "                    \"You're amazing. You're qualified, and you're ready\",\n",
    "                    \"by far the most honest and innovative candidate with real solutions\",\n",
    "                    \"HE IS PROGRESSIVE\",\n",
    "                    \"by far the most honest and innovative candidate with real solutions\",\n",
    "                    \"Yeah it's called lying she does it very well if you don't remember correctly last year she was a native American off with her head she is the scum of the Earth\",\n",
    "                    \"0 votes for you during those elections\"\n",
    "                    \"Yes, somehow you war criminals manage to get away with it every time.\",\n",
    "                    \"Get 'em Joe.\"\n",
    "                   ]\n",
    "political_tweets_proc = list(map(preprocess_tweet, political_tweets))\n",
    "pol_seqs = tokenizer.texts_to_sequences(political_tweets_proc)\n",
    "pol_seqs_padded = pad_sequences(pol_seqs, maxlen=max_length1)\n",
    "lstm_model5.predict(pol_seqs_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_json = tokenizer.to_json()\n",
    "with io.open('tokenizer_200k.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(tokenizer_json, ensure_ascii=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
